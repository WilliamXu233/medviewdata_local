{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 23:14:00,750 - INFO: Starting advanced domain-confined scraping from ://\n",
      "2025-02-24 23:14:00,752 - INFO: Scraping: :// (Depth: 0)\n",
      "2025-02-24 23:14:00,758 - ERROR: Request error scraping ://: No connection adapters were found for '://'\n",
      "2025-02-24 23:14:00,760 - INFO: Scraping complete. Total unique links found: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Links:\n",
      "\n",
      "Total unique links found: 0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Normalize relative links into absolute URLs\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import logging\n",
    "import re\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Class that recursively scrapes all internal links from a given base URL\n",
    "class AdvancedWebScraper:\n",
    "# Initialize base URL, depth limit, timeout, and link tracking\n",
    "    def __init__(self, base_url, max_depth=5, timeout=15):\n",
    "        \"\"\"\n",
    "        Initialize an advanced web scraper with comprehensive link extraction\n",
    "        \n",
    "        :param base_url: Starting URL to scrape\n",
    "        :param max_depth: Maximum depth of recursive scraping\n",
    "        :param timeout: Request timeout in seconds\n",
    "        \"\"\"\n",
    "        # Normalize the base URL\n",
    "        parsed_base = urlparse(base_url)\n",
    "        self.base_domain = parsed_base.netloc.lower()\n",
    "        self.base_scheme = parsed_base.scheme\n",
    "# Define the base URL to start scraping (sensitive info redacted)\n",
    "base_url = \"<BASE_URL>\"  # REPLACE with target domain\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.timeout = timeout\n",
    "        self.visited_urls = set()\n",
    "        self.all_links = set()\n",
    "        \n",
    "        # Configure logging\n",
    "# Configure logging to capture status and errors\n",
    "        logging.basicConfig(level=logging.INFO, \n",
    "                            format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create a robust session with retry mechanism\n",
    "# Create a requests session with retry configuration\n",
    "        self.session = requests.Session()\n",
    "# Set up HTTP retry strategy to handle connection errors gracefully\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=0.3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=['GET', 'HEAD']\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "    \n",
    "    def normalize_url(self, url):\n",
    "        \"\"\"\n",
    "        Normalize URLs to remove tracking parameters, fragments, etc.\n",
    "        \n",
    "        :param url: URL to normalize\n",
    "        :return: Normalized URL\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            # Remove fragment and query parameters related to tracking\n",
    "            cleaned_parsed = parsed._replace(\n",
    "                fragment='',\n",
    "                query=re.sub(r'(utm_\\w+|ref|track)=[^&]*', '', parsed.query)\n",
    "            )\n",
    "            normalized = urlunparse(cleaned_parsed)\n",
    "            \n",
    "            # Remove trailing slash for consistency\n",
    "            return normalized.rstrip('/')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error normalizing URL {url}: {e}\")\n",
    "            return url\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"\n",
    "        Comprehensive URL validation\n",
    "        \n",
    "        :param url: URL to validate\n",
    "        :return: Boolean indicating if URL is valid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            \n",
    "            # Extensive filtering\n",
    "            return (\n",
    "                parsed.scheme in ['http', 'https'] and \n",
    "                parsed.netloc.lower() == self.base_domain and\n",
    "                not any(ext in url.lower() for ext in [\n",
    "                    '.pdf', '.jpg', '.jpeg', '.png', '.gif', \n",
    "                    '.css', '.js', '.xml', '.svg', '.ico', \n",
    "                    '.mp4', '.mp3', '.zip', '.csv', '.xls'\n",
    "                ]) and\n",
    "                not re.search(r'(#|@|\\?)', url)  # Exclude anchors, emails, complex queries\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error validating URL {url}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_links(self, html_content, base_url):\n",
    "        \"\"\"\n",
    "        Advanced link extraction method\n",
    "        \n",
    "        :param html_content: HTML content of the page\n",
    "        :param base_url: Base URL of the current page\n",
    "        :return: Set of extracted links\n",
    "        \"\"\"\n",
    "        links = set()\n",
    "        \n",
    "        try:\n",
    "            # Use BeautifulSoup for parsing\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Extract links from various sources\n",
    "            link_sources = [\n",
    "                soup.find_all('a', href=True),  # Standard links\n",
    "                soup.find_all('link', href=True),  # Link tags\n",
    "                soup.find_all(attrs={'data-href': True}),  # Custom data attributes\n",
    "            ]\n",
    "            \n",
    "            for source in link_sources:\n",
    "                for link in source:\n",
    "                    # Get href or data-href attribute\n",
    "                    href = link.get('href') or link.get('data-href', '')\n",
    "                    \n",
    "                    # Convert to absolute URL\n",
    "# Normalize relative links into absolute URLs\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    # Normalize and validate\n",
    "                    normalized_url = self.normalize_url(full_url)\n",
    "                    \n",
    "                    if self.is_valid_url(normalized_url):\n",
    "                        links.add(normalized_url)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting links from {base_url}: {e}\")\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def scrape_page(self, url, depth=0):\n",
    "        \"\"\"\n",
    "        Recursive page scraping with advanced error handling\n",
    "        \n",
    "        :param url: URL to scrape\n",
    "        :param depth: Current recursion depth\n",
    "        \"\"\"\n",
    "        # Avoid exceeding max depth or revisiting URLs\n",
    "        if (depth > self.max_depth or \n",
    "            url in self.visited_urls):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Mark URL as visited\n",
    "            self.visited_urls.add(url)\n",
    "            self.logger.info(f\"Scraping: {url} (Depth: {depth})\")\n",
    "            \n",
    "            # Fetch webpage with robust session\n",
    "            response = self.session.get(\n",
    "                url, \n",
    "                timeout=self.timeout, \n",
    "                headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Extract and process links\n",
    "            page_links = self.extract_links(response.text, url)\n",
    "            \n",
    "            for link in page_links:\n",
    "                # Add unique links\n",
    "                if link not in self.all_links:\n",
    "                    self.all_links.add(link)\n",
    "                    \n",
    "                    # Recursive scraping\n",
    "                    self.scrape_page(link, depth + 1)\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            self.logger.error(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error scraping {url}: {e}\")\n",
    "    \n",
    "    def start_scraping(self):\n",
    "        \"\"\"\n",
    "        Initiate the comprehensive web scraping process\n",
    "        \n",
    "        :return: Set of unique links within the domain\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting advanced domain-confined scraping from {self.base_url}\")\n",
    "        self.scrape_page(self.base_url)\n",
    "        \n",
    "        self.logger.info(f\"Scraping complete. Total unique links found: {len(self.all_links)}\")\n",
    "        return self.all_links\n",
    "# Example usage\n",
    "def main():\n",
    "    # Example website to scrape (replace with your target website)\n",
    "# Define the base URL to start scraping (sensitive info redacted)\n",
    "base_url = \"<BASE_URL>\"  # REPLACE with target domain\n",
    "    \n",
    "    # Create scraper instance\n",
    "# Initialize scraper with base URL and desired depth\n",
    "    scraper = AdvancedWebScraper(base_url, max_depth=50)\n",
    "    \n",
    "    # Start scraping and get links\n",
    "    links = scraper.start_scraping()\n",
    "    \n",
    "    # Print all discovered links\n",
    "    print(\"Discovered Links:\")\n",
    "    for link in sorted(links):\n",
    "        print(link)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTotal unique links found: {len(links)}\")\n",
    "    print(links)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_excel_column(file_path, sheet_name, column_name):\n",
    "    \"\"\"\n",
    "    Extract data from a specific column in an Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file\n",
    "    sheet_name (str): Name of the sheet to extract data from\n",
    "    column_name (str): Name of the column to extract data from\n",
    "    \n",
    "    Returns:\n",
    "    list: List containing the values from the specified column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        \n",
    "        # Check if the column exists\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"Error: Column '{column_name}' not found in sheet '{sheet_name}'.\")\n",
    "            return []\n",
    "        \n",
    "        # Extract data from the specified column into a list\n",
    "        column_data = df[column_name].tolist()\n",
    "        \n",
    "        return column_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:\\\\Users\\\\hahaha\\\\Downloads\\\\MVS Top 10 Instructions.xlsx\"  # Replace with your Excel file path\n",
    "    sheet_name = \"Top 10\"                 # Replace with your sheet name\n",
    "    column_name = \"URL\"               # Replace with your column name\n",
    "    \n",
    "    urls = extract_excel_column(file_path, sheet_name, column_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.medviewsystems.com//freestyle-lite/', 'https://www.medviewsystems.com//iv-piggy-back-procedure/', 'https://www.medviewsystems.com//picc-line-dressing-change-with-biopatch-and-griplock/', 'https://www.medviewsystems.com//sapphire-pump-infusion-pump/', 'https://www.medviewsystems.com//cadd-solis-ambulatory-infusion-pump/', 'https://www.medviewsystems.com//freedom-60-infusionspumpe-syringe-infusion-system/', 'https://www.medviewsystems.com//invacare-g-series-bed/', 'https://www.medviewsystems.com//salter-aire-elite-compressor/', 'https://www.medviewsystems.com//airsense-11-resmed-cpap/', 'https://www.medviewsystems.com//nuvo-lite-mark-5-oxygen-concentrator-gce/']\n"
     ]
    }
   ],
   "source": [
    "len(urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All responses have been appended to medviewrag_demo.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# List of URLs to send requests to\n",
    "headers = {\n",
    "    'Authorization': 'jina_e84e30931c6f42df9f2850db7d500612JKXdKvo9ldrvdDbYyEfI0kQSOb6A',\n",
    "    'X-Return-Format': 'markdown'\n",
    "}\n",
    "\n",
    "# Path for the output text file\n",
    "file_path = 'medviewrag_demo.txt'\n",
    "\n",
    "# Open the file in append mode ('a') to add new data without overwriting existing content\n",
    "with open(file_path, 'a') as txt_file:\n",
    "    # Loop through each URL and make a GET request\n",
    "    for url in urls:\n",
    "        url = 'https://r.jina.ai/'+url\n",
    "        response = requests.get(url,headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # If the request was successful, write the response text to the file\n",
    "            # You can directly write the JSON string, or convert it to text (string format)\n",
    "            txt_file.write(f\"Response from {url}:\\n\")\n",
    "            txt_file.write(response.text)  # Write the response as plain text\n",
    "            txt_file.write(\"\\n\\n\")  # Add some spacing between responses for readability\n",
    "        else:\n",
    "            # Handle failed requests by writing a message\n",
    "            txt_file.write(f\"Failed to retrieve data from {url} with status code {response.status_code}\\n\\n\")\n",
    "\n",
    "print(f\"All responses have been appended to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0928env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
